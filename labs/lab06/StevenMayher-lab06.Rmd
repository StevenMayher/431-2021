---
title: "Steven Mayher: Lab 06 for PQHS 431"
date: "Due: 2021-11-08 | Last Edit: `r Sys.time()`"
output:
    html_document:
    toc: yes
    toc_float: yes
    code_folding: show
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

```{r, warning = FALSE, message = FALSE}
library(MatchLinReg)
library(simputation) # for single impuation
library(car) # for boxCox
library(GGally) # for ggpairs
library(ggrepel) # help with residual plots
library(equatiomatic) # help with equation extraction
library(knitr)
library(janitor)
library(magrittr)
library(patchwork)
library(broom)
library(naniar)
library(tidyverse)

theme_set(theme_bw())
```

# The Data for Lab 06

The above code below ingests the data as necessary for the lab:

```{r}
data("lindner")

lindner_alive <- lindner %>%
    filter(sixMonthSurvive == 1) %>%
    mutate(id = row_number()) %>%
    as_tibble()

set.seed(431)
lindner_alive_train = lindner_alive %>% sample_frac(0.7)
lindner_alive_test = lindner_alive %>% anti_join(lindner_alive_train, by = "id")
```


# Question 1 (20 points)

In order to determine which power transformation for the outcome would be preferred for the `cardbill` outcome (i.e. the 6-month cardiac-related costs) for surviving patients using the `lindner_alive_train` data set, the code below has been used to examine both the log transformation and the square-root transformation:

```{r}
lindner_alive_train = lindner_alive_train %>%
  mutate(ln_cardbill = log(cardbill))

p1 <- ggplot(lindner_alive_train, aes(x = ln_cardbill)) +
  geom_histogram(bins = nclass.scott(na.omit(lindner_alive_train$cardbill)), fill = "slateblue", col = "white")

p2 <- ggplot(lindner_alive_train, aes(sample = ln_cardbill)) +
  geom_qq(col = "slateblue") + geom_qq_line(col = "red")

p3 <- ggplot(lindner_alive_train, aes(x = "", y = ln_cardbill)) +
  geom_violin(fill = "slateblue", alpha = 0.3) +
  geom_boxplot(fill = "slateblue", width = 0.3, outlier.color = "red") +
  labs(x = "") + coord_flip()

p1 + p2 - p3 +
  plot_layout(ncol = 1, height = c(3, 2)) +
  plot_annotation(title = "Log of the 6-month cardiac-related costs cardbills values ($)", subtitle = paste0("Model Development Sample: ", nrow(lindner_alive_train), " surviving adults, log transformation"))

lindner_alive_train = lindner_alive_train %>%
  mutate(sqrt_cardbill = sqrt(cardbill))

p4 <- ggplot(lindner_alive_train, aes(x = sqrt_cardbill)) +
  geom_histogram(bins = nclass.scott(na.omit(lindner_alive_train$cardbill)), fill = "slateblue", col = "white")

p5 <- ggplot(lindner_alive_train, aes(sample = sqrt_cardbill)) +
  geom_qq(col = "slateblue") + geom_qq_line(col = "red")

p6 <- ggplot(lindner_alive_train, aes(x = "", y = sqrt_cardbill)) +
  geom_violin(fill = "slateblue", alpha = 0.3) +
  geom_boxplot(fill = "slateblue", width = 0.3, outlier.color = "red") +
  labs(x = "") + coord_flip()

p4 + p5 - p6 +
  plot_layout(ncol = 1, height = c(3, 2)) +
  plot_annotation(title = "Square-Root of the 6-month cardiac-related costs cardbills values ($)", subtitle = paste0("Model Development Sample: ", nrow(lindner_alive_train), " surviving adults"))
```

Between the two possible transformations of the outcome as seen in the plots above, the log transformation of the outcome fits the data better. If we take into account the predictor `ejecfrac` with a Box-Cox plot and/or the `powerTransform` function (both from the `car` library) as well, the log transformation is once again shown to be the best transformation option between the two available options, both of which are shown below:

```{r}
boxCox(lindner_alive_train$cardbill ~ lindner_alive_train$ejecfrac)
powerTransform(lindner_alive_train$cardbill ~ lindner_alive_train$ejecfrac)
```

With this knowledge, a linear model using the log transformation was created with the code below:

```{r}
model1 = lm(ln_cardbill ~ ejecfrac, data = lindner_alive_train)
```

The exact equation for this model with the model's coefficients is as follows:

```{r}
equatiomatic::extract_eq(model1, use_coefs = TRUE, coef_digits = 3)
```

```{r}
tidy_model1 <- tidy(model1, conf.int = TRUE, conf.level = 0.95)

tidy_model1 %>%
  select(term, estimate, std.error, p.value, conf.low, conf.high) %>%
  knitr::kable(digits = 4)
```

The model's R-Squared, adjusted R-Squared, sigma, AIC, and BIC values are as follows:

```{r}
glance(model1) %>%
  mutate(name = "model1") %>%
  select(name, r.squared, adj.r.squared, sigma, AIC, BIC) %>%
  knitr::kable(digits = c(0, 3, 3, 3, 0, 0))
```

To interpret and contextualize the results of this regression, there are four assumptions that should be assessed - linearity, constant variance (i.e. homoscedasticity), normality, and independence. The first three assumptions - linearity, constant variance, and normality - can be assessed with residual plots. The Residuals vs Fitted plot can be used to assess linearity, the Normal Q-Q can be used to assess normality, and the Scale-Location plot can be used to assess constant variance, and all three are shown below:

```{r}
plot(model1, which = 1)
```
From the Residuals vs Fitted plot, I'd conclude that there are not any serious issues with non-linearity, as there isn't any substantial bends in the smooth line fitted to this plot.

```{r}
plot(model1, which = 3)
```

As with assessing a Residuals vs Fitted plot, a Scale-Location plot with a relatively flat loess smooth line indicates constant variance. While there is some small bend in the center of the plot, there is no meaningful trend up or down, so I'd conclude that there's likely no issues with constant variance in this plot either.

```{r}
plot(model1, which = 2)
```

The Normal Q-Q plot above does indicate some issues with normality with this model, however given that the ideal transformation of this data as indicated by the Box-Cox plot above indicates that the -square-root transformation would be ideal for this model, not the log transformation, this isn't entirely unexpected. The outliers could be further examined by utilizing the `outlierTest` function to investigate their impact on the normality, however per instruction from Piazza this step will not be examined at this time.

To assess the final assumption, independence, we can re-examine the first two residual plots. If the data is truly taken at random for an experiment, then the residuals should appear to be randomly scattered and show no trends, clumps, or other patterns when plotted against the predictor values, and their clearly appears to be some incremental binning pattern in both the Residuals vs Fitted and Scale-Location plots, so their is likely an issue with dependence with this data.

# Question 2 (10 points)

To examine the effect of this third variable, `abcix`, along with the original predictor `ejecfrac` with the transformed outcome data from the first question, the following model was created:

```{r}
model2 = lm(ln_cardbill ~ ejecfrac + abcix, data = lindner_alive_train)
```

The exact equation for this model with the model's coefficients is as follows:

```{r}
equatiomatic::extract_eq(model2, use_coefs = TRUE, coef_digits = 3)
```

```{r}
tidy_model2 <- tidy(model2, conf.int = TRUE, conf.level = 0.95)

tidy_model2 %>%
  select(term, estimate, std.error, p.value, conf.low, conf.high) %>%
  knitr::kable(digits = 4)
```

The model's R-Squared, adjusted R-Squared, sigma, AIC, and BIC values are as follows:

```{r}
glance(model2) %>%
  mutate(name = "model2") %>%
  select(name, r.squared, adj.r.squared, sigma, AIC, BIC) %>%
  knitr::kable(digits = c(0, 3, 3, 3, 0, 0))
```

As done with `model1` in the answer to Question 1 above, the linearity, constant variance, normality, and independence of `model2` have been assessed with the following plots below to interpret and contextualize results of this new regression:

```{r}
plot(model2, which = 1)
```
From this Residuals vs Fitted plot, I'd conclude that there are not any serious issues with non-linearity in this new model, as there are no significant bends in the smooth line fitted to this plot.

```{r}
plot(model2, which = 3)
```

While there is some small bend in the center of this Scale-Location plot, to a lesser degree than the plot for `model1`, there is once again no meaningful trend up or down, so I'd conclude that there's likely no issues with constant variance in this plot, and thus in `model2` either.

```{r}
plot(model2, which = 2)
```

As with the Normal Q-Q plot for `model1`, this Normal Q-Q plot for `model2` does indicate some issues with normality as well. Again, given that the ideal transformation of this data as indicated by the Box-Cox plot in Question 1 above indicates that the -square-root transformation would be ideal for this model, not the log transformation, this isn't entirely unexpected that a new model with additional predictor would stray far from this suggested transformation. The Normal Q-Q plot for this model in particular appears to have a little more right skew than the plot for `model1`. Again, some of the extreme outliers could be further examined by utilizing the `outlierTest` function, however per instruction from Piazza this step will not be examined at this time.

Lastly, given that `model2` was created by simply adding an additional predictor variable (i.e. `abcix`) to the model used for Question 1, we can conclude that we will likely have issues with the linearity assumption again. Inspection of the Residuals vs Fitted plot does confirm this though, this time with some clumping of values appearing alongside the apparent binning pattern that was also observed in the Residuals vs Fitted plot of `model1`. As such, their is likely still an issue with the independence with this data.

# Question 3 (10 points)

While there isn't really an ideal way to perform variable selection (at least as far as we have been instructed in 431), as Dr. Love has indicated in the lecture slides that "there is an enormous amount of evidence that variable selection causes severe problems in estimation and inference", with stepwise regression being considered particularly egregious, there really isn't many better options at this time. As such, a model was created below by adding the `stent`, `height`, `female`, `diabetic`, `acutemi`, and `ves1proc` variables as predictors to the same model used for Question 2 (i.e. `model2`) to create a new temporary model for this Question, so a stepwise regression could be performed to determine which predictors should and shouldn't be kept. First, the model was created as seen below:

```{r}
model3 = lm(ln_cardbill ~ ejecfrac + abcix + stent + height + female + diabetic + acutemi + ves1proc, data = lindner_alive_train)
```

The exact equation for this model with the model's coefficients is as follows:

```{r}
equatiomatic::extract_eq(model3, use_coefs = TRUE, coef_digits = 3)
```

```{r}
tidy_model3 <- tidy(model3, conf.int = TRUE, conf.level = 0.95)

tidy_model3 %>%
  select(term, estimate, std.error, p.value, conf.low, conf.high) %>%
  knitr::kable(digits = 4)
```

The model's R-Squared, adjusted R-Squared, sigma, AIC, and BIC values are as follows:

```{r}
glance(model3) %>%
  mutate(name = "model2") %>%
  select(name, r.squared, adj.r.squared, sigma, AIC, BIC) %>%
  knitr::kable(digits = c(0, 3, 3, 3, 0, 0))
```

The stepwise analysis is as follows:

```{r}
step(model3)
```

For clarification, stepwise regression examines how the AIC value for a model changes when removing a predictor, removes the predictor that causes the AIC value to decrease the most, and then repeats until the AIC value doesn't decrease anymore from removing predictors. With this in mind, the stepwise regression suggests that the `height` and `diabetic` variables be removed from the model.

One of the few other options that can be used to further examine these results and check for correlation issues specifically is to examine the **variance inflation factor**, as seen below:

```{r}
vif(model3)
```

None of the values above exceed 5, so there doesn't appear to be any problem with collinearity here.

As done with `model1` and `model2` in the answers to Questions 1 and 2 above, the linearity, constant variance, normality, and independence of `model3` have been assessed with the following plots below to interpret and contextualize results of this new regression:

```{r}
plot(model3, which = 1)
```

As with the first two models, the Residuals vs Fitted plot for `model3` doesn't appear to indicate any issues with linearity in this new model.

```{r}
plot(model3, which = 3)
```

Again, as with the Scale-Location plots for the first two models, there doesn't appear to be any significant bend in the smooth in the Scale-Location plot for `model3`, indicating that there is likely no issue with constant variance for this model.

```{r}
plot(model3, which = 2)
```

As with the previous two models, `model3` again appears to display some issues with normality, however this could once again likely be mitigated by choosing a better power transformation for the model than the permitted choices allowed.

Lastly, unlike the first two models, the Residuals vs Fitted and Scale-Location plots both appear to show much less trends, patterns, and / or clumps than the first two models, however I'm still not certain as to whether or not I'd say it passes the independence assumption in its current state.

# Question 4 (10 points)

The necessary code for creating a new model based off of `model3` that uses an interaction term between `height` and `female` has been provided below:

```{r}
model4 = lm(ln_cardbill ~ ejecfrac + stent + height * female + diabetic + acutemi + ves1proc, data = lindner_alive_train)
```

The exact equation for this model with the model's coefficients is as follows:

```{r}
equatiomatic::extract_eq(model4, use_coefs = TRUE, coef_digits = 3)
```

```{r}
tidy_model4 <- tidy(model2, conf.int = TRUE, conf.level = 0.95)

tidy_model4 %>%
  select(term, estimate, std.error, p.value, conf.low, conf.high) %>%
  knitr::kable(digits = 4)
```

The model's R-Squared, adjusted R-Squared, sigma, AIC, and BIC values are as follows:

```{r}
glance(model4) %>%
  mutate(name = "model4") %>%
  select(name, r.squared, adj.r.squared, sigma, AIC, BIC) %>%
  knitr::kable(digits = c(0, 3, 3, 3, 0, 0))
```

As done with `model1`, `model2`, and `model3` in the answers to Questions 1, 2, and 3 above, the linearity, constant variance, normality, and independence of `model4` have been assessed with the following plots below to interpret and contextualize results of this new regression:

```{r}
plot(model4, which = 1)
```

As with the first three models, the Residuals vs Fitted plot for `model4` doesn't appear to indicate any issues with linearity in this new model.

```{r}
plot(model4, which = 3)
```

Again, as with the Scale-Location plots for the first two models, there doesn't appear to be any substantial bend in the smooth in the Scale-Location plot for `model4`, indicating that there is likely no issue with constant variance for this model.

```{r}
plot(model4, which = 2)
```

As with the previous three models, `model4` again appears to display some issues with normality, however this could once again likely be mitigated by choosing a better power transformation for the model than the permitted choices allowed.

Lastly, unlike the first two models, the Residuals vs Fitted and Scale-Location plots both appear to show much less trends, patterns, and / or clumps than the first two models, just as seen with `model3`. However I'm still not certain as to whether or not I'd say it passes the independence assumption in its current state.

# Question 5 (20 points)

The necessary code for creating the new tibbles needed to analyze the MAPE, RMSPE, and maximum prediction error from the test data is provided below:

```{r}
lindner_alive_test = lindner_alive_test %>%
  mutate(ln_cardbill = log(cardbill))


model1_test = augment(model1, newdata = lindner_alive_test) %>%
  mutate(name = "model1", fit_cardbill = exp(.fitted), res_cardbill = cardbill - fit_cardbill)

model2_test =  augment(model2, newdata = lindner_alive_test) %>%
  mutate(name = "model2", fit_cardbill = exp(.fitted), res_cardbill = cardbill - fit_cardbill)

model3_test =  augment(model3, newdata = lindner_alive_test) %>%
  mutate(name = "model3", fit_cardbill = exp(.fitted), res_cardbill = cardbill - fit_cardbill)

model4_test =  augment(model4, newdata = lindner_alive_test) %>%
  mutate(name = "model4", fit_cardbill = exp(.fitted), res_cardbill = cardbill - fit_cardbill)
```

The code to produce the tables comparing the results across the models is provided below:

```{r}
bind_rows(glance(model1), glance(model2), glance(model3), glance(model4)) %>%
  mutate(Names = c("model1", "model2", "model3", "model4")) %>%
  select(Names, r2 = r.squared, adj_r2 = adj.r.squared, sigma, AIC, BIC, df, df_res = df.residual) %>%
  kable(digits = c(0, 4, 4, 5, 1, 0, 0, 0))

test_comp = bind_rows(model1_test, model2_test, model3_test, model4_test) %>%
  arrange(id, name)

test_comp %>%
  group_by(name) %>%
  summarize(n = n(),
            MAPE = mean(abs(res_cardbill)),
            RMSPE = sqrt(mean(res_cardbill^2)),
            max_error = max(abs(res_cardbill))) %>%
  kable(digits = c(0, 0, 4, 3, 2))
```

In the training sample, `model3` has the best R-square, adjusted R-square, sigma, and AIC values, however `model2` has the best BIC value.

In the data from the withheld test sample though, `model3` has the best (i.e. smallest) mean APE (MAPE) and RMSPE values, however `model4` has the smallest maximum predictive error.

As such, in both the training and test samples, `model3` performed the best overall.

- *It's worth noting that I do find the incredibly large values for the MAPE, RMSPE, and max_error concerning, however I genuinely cannot find anything incorrect about my analysis, so if I have made an error here please let me know.*

# Question 6 (20 points)

What we've done in this assignment is work through a simple linear regression, visualizing and interpreting the role of a potential cofounder, adjusting for this cofounder, expanding the model, and comparing the resultant models created at each step of the process to draw conclusions about them. There are a number of ways that we could relate what was done in this assignment back to Spiegelhalter text, however the one that stands out the most to me is the importance of transparency and rigorous statistical analysis when examining experimental data, as discussed in chapter 10 of Spiegelhalter. It is important to examine multiple different possible predictors and cofounders, perform proper transformations of the data, and maintain transparency with the data, analyses, and results, among many other factors when creating models of any kind (not just linear models) and reporting their results. For example, failing to consider multiple impacting variables on a model can lead to incorrect conclusions about the associated data, which even if done unintentionally can lead to statistical fraud, which is discussed at length in chapter 10 of Spiegelhalter.

## Session Information

```{r}
sessionInfo()
```
